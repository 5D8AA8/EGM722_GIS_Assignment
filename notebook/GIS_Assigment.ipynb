{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddbc3632-47d7-4739-93d5-5170eec14800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # lets you interact with the operating system\n",
    "import tkinter as tk #loads the Tkinter GUI\n",
    "from tkinter import filedialog # lets you open a standard file dialog (like the one that pops up to pick a file) inside your Tkinter GUI.\n",
    "import pandas as pd #imports pandas which allows you to read the CSV etc\n",
    "import geopandas as gpd #imports geopandas which allows you to read geo files\n",
    "import folium # A Python wrapper for Leaflet.js, used to create interactive web maps (saved as HTML)\n",
    "import matplotlib.pyplot as plt #Used for making static visual plots and maps\n",
    "import shapely #Imports the whole Shapely library, which handles all geometric objects\n",
    "from shapely.geometry import mapping # Converts Shapely geometry objects (like Point, Polygon) into a dictionary format \n",
    "from shapely.geometry import Point, Polygon, LineString\n",
    "from shapely.geometry import MultiPolygon #This is used when a feature (like a country) has multiple disconnected polygons\n",
    "import webbrowser #Lets you open URLs in the default browser\n",
    "import earthaccess #A NASA tool to search and download Earth data\n",
    "from shapely.geometry.polygon import orient #This function ensures a polygon's vertices follow a consistent winding order (clockwise or counter-clockwise), required for correct processing or display.\n",
    "import rasterio # main library for reading datasets such as.tif\n",
    "from rasterio.plot import show #used to display and plot datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5573cb3-ec7e-411b-b70e-0c6b3a930ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"GDAL_DATA\"] = r\"C:\\Users\\marti\\.conda\\envs\\Spatial_cluster\\Library\\share\\gdal\"\n",
    "#os.environ[\"PROJ_LIB\"] = r\"C:\\Users\\marti\\.conda\\envs\\Spatial_cluster\\Library\\share\\proj\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbef20ab-1cc1-4f5e-866f-5e923aa91495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the csv file use a dialog box (from TKinter user guide) latin 1 is useful if there are special charachters in the file. Note: You could also try 'utf-8' \n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "file = filedialog.askopenfilename()\n",
    "df = pd.read_csv(file, encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46cb4e1-b68c-4eeb-9c1a-995d22464fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of the data:\n",
      "     ï»¿eventid  iyear  imonth  iday approxdate  extended  resolution  \\\n",
      "0  1.970010e+11   1970       1     0        NaN         0         NaN   \n",
      "1  1.970030e+11   1970       3    31        NaN         1  03/04/1970   \n",
      "2  1.971110e+11   1971      11    20        NaN         0         NaN   \n",
      "3  1.973040e+11   1973       4    25        NaN         0         NaN   \n",
      "4  1.973080e+11   1973       8    29        NaN         0         NaN   \n",
      "5  1.974070e+11   1974       7    15        NaN         0         NaN   \n",
      "6  1.974080e+11   1974       8    15        NaN         0         NaN   \n",
      "7  1.974110e+11   1974      11    14        NaN         0         NaN   \n",
      "8  1.974110e+11   1974      11    22        NaN         0         NaN   \n",
      "9  1.975020e+11   1975       2    28        NaN         0         NaN   \n",
      "\n",
      "   country  country_txt  region region_txt provstate      city   latitude  \\\n",
      "0      101        Japan       4  East Asia   Fukouka   Fukouka  33.580412   \n",
      "1      101        Japan       4  East Asia   Fukouka   Fukouka  33.580412   \n",
      "2      201       Taiwan       4  East Asia    Penghu   Unknown  23.583333   \n",
      "3      101        Japan       4  East Asia  Kanagawa  Yokosuka  35.281341   \n",
      "4      101        Japan       4  East Asia     Tokyo     Tokyo  35.689125   \n",
      "5      101        Japan       4  East Asia     Hyogo     Itami  34.784306   \n",
      "6      184  South Korea       4  East Asia     Seoul     Seoul  37.566535   \n",
      "7      101        Japan       4  East Asia     Tokyo     Tokyo  35.689125   \n",
      "8      101        Japan       4  East Asia   Fukouka   Fukouka  33.580412   \n",
      "9      101        Japan       4  East Asia     Tokyo     Tokyo  35.689125   \n",
      "\n",
      "    longitude  specificity  vicinity location summary  \n",
      "0  130.396361            1         0      NaN     NaN  \n",
      "1  130.396361            1         0      NaN     NaN  \n",
      "2  119.583330            4         0      NaN     NaN  \n",
      "3  139.672200            1         0      NaN     NaN  \n",
      "4  139.747742            1         0      NaN     NaN  \n",
      "5  135.400947            1         0      NaN     NaN  \n",
      "6  126.977969            1         0      NaN     NaN  \n",
      "7  139.747742            1         0      NaN     NaN  \n",
      "8  130.396361            1         0      NaN     NaN  \n",
      "9  139.747742            1         0      NaN     NaN  \n",
      "\n",
      "-----------------------------\n",
      "\n",
      "Total number of rows: 847\n",
      "Total number of columns: 19\n",
      "\n",
      "Missing values per column:\n",
      "ï»¿eventid       0\n",
      "iyear            0\n",
      "imonth           0\n",
      "iday             0\n",
      "approxdate     845\n",
      "extended         0\n",
      "resolution     839\n",
      "country          0\n",
      "country_txt      0\n",
      "region           0\n",
      "region_txt       0\n",
      "provstate        0\n",
      "city             0\n",
      "latitude         8\n",
      "longitude        8\n",
      "specificity      0\n",
      "vicinity         0\n",
      "location       715\n",
      "summary        591\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# load the csv into a dataframe df\n",
    "# Show the first 10 rows of the csv dataset  - to let you understand what is in it if there are any unusal symbols etc\n",
    "print(\"First 10 rows of the data:\")\n",
    "print(df.head(10))\n",
    "\n",
    "#seperator to make the output clearer, puts in a new line before and after.\n",
    "print(\"\\n-----------------------------\\n\")\n",
    "\n",
    "# Count how many rows are in the data, this will let you know what happens when it is cleaned with the dataframe df\n",
    "row_count = len(df)\n",
    "print(\"Total number of rows:\", row_count)\n",
    "\n",
    "# Count how many columns are in the df so that you can understand the size and shape of the csv dataset\n",
    "col_count = len(df.columns)\n",
    "print(\"Total number of columns:\", col_count)\n",
    "\n",
    "#check for missing values per column\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8da08040-1e45-4c1d-8f25-1e3370b0a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check and to reduce the three date colums into one to limit the impact processing.  \n",
    "if 'iyear' in df and 'imonth' in df and 'iday' in df:\n",
    "    #combine the year, month and day into a single column\n",
    "    df['date'] = pd.to_datetime(\n",
    "        #the error coerce means that it will set not a time NaT rather than cause an error if the value is invalid\n",
    "        dict(year=df['iyear'], month=df['imonth'], day=df['iday']), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9109f430-2757-47d8-9461-ed1beb561351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ï»¿eventid  iyear  imonth  iday approxdate  extended  resolution  \\\n",
      "0  1.970010e+11   1970       1     0        NaN         0         NaN   \n",
      "1  1.970030e+11   1970       3    31        NaN         1  03/04/1970   \n",
      "2  1.971110e+11   1971      11    20        NaN         0         NaN   \n",
      "3  1.973040e+11   1973       4    25        NaN         0         NaN   \n",
      "4  1.973080e+11   1973       8    29        NaN         0         NaN   \n",
      "5  1.974070e+11   1974       7    15        NaN         0         NaN   \n",
      "6  1.974080e+11   1974       8    15        NaN         0         NaN   \n",
      "7  1.974110e+11   1974      11    14        NaN         0         NaN   \n",
      "8  1.974110e+11   1974      11    22        NaN         0         NaN   \n",
      "9  1.975020e+11   1975       2    28        NaN         0         NaN   \n",
      "\n",
      "   country  country_txt  region region_txt provstate      city   latitude  \\\n",
      "0      101        Japan       4  East Asia   Fukouka   Fukouka  33.580412   \n",
      "1      101        Japan       4  East Asia   Fukouka   Fukouka  33.580412   \n",
      "2      201       Taiwan       4  East Asia    Penghu   Unknown  23.583333   \n",
      "3      101        Japan       4  East Asia  Kanagawa  Yokosuka  35.281341   \n",
      "4      101        Japan       4  East Asia     Tokyo     Tokyo  35.689125   \n",
      "5      101        Japan       4  East Asia     Hyogo     Itami  34.784306   \n",
      "6      184  South Korea       4  East Asia     Seoul     Seoul  37.566535   \n",
      "7      101        Japan       4  East Asia     Tokyo     Tokyo  35.689125   \n",
      "8      101        Japan       4  East Asia   Fukouka   Fukouka  33.580412   \n",
      "9      101        Japan       4  East Asia     Tokyo     Tokyo  35.689125   \n",
      "\n",
      "    longitude  specificity  vicinity location summary       date  \n",
      "0  130.396361            1         0      NaN     NaN        NaT  \n",
      "1  130.396361            1         0      NaN     NaN 1970-03-31  \n",
      "2  119.583330            4         0      NaN     NaN 1971-11-20  \n",
      "3  139.672200            1         0      NaN     NaN 1973-04-25  \n",
      "4  139.747742            1         0      NaN     NaN 1973-08-29  \n",
      "5  135.400947            1         0      NaN     NaN 1974-07-15  \n",
      "6  126.977969            1         0      NaN     NaN 1974-08-15  \n",
      "7  139.747742            1         0      NaN     NaN 1974-11-14  \n",
      "8  130.396361            1         0      NaN     NaN 1974-11-22  \n",
      "9  139.747742            1         0      NaN     NaN 1975-02-28  \n"
     ]
    }
   ],
   "source": [
    "# Show the first 10 rows to check the date has been combined in the data frame\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fd0b66f-6de5-4bad-8080-0daea3fae6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of rows: 847\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows to show how many incidents there are\n",
    "print(\"\\nTotal number of rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c08fe116-bdcc-4251-bbb0-4b301621f17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of columns: 20\n"
     ]
    }
   ],
   "source": [
    "# Count how many columns are in the csv dataset\n",
    "col_count = len(df.columns)\n",
    "print(\"Total number of columns:\", col_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3b724b9-e661-47da-8090-5e4452713388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the columns that are needed for the project in a list[]\n",
    "columns_to_keep = ['region_txt', 'country_txt','city',  'date', 'latitude', 'longitude', 'summary']\n",
    "#df = df[columns_to_keep] this tells Pandas to only keep these columns\n",
    "df = df[columns_to_keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a497a98-b370-459f-8d2a-712585bb8fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of columns: 7\n",
      "  region_txt  country_txt      city       date   latitude   longitude summary\n",
      "0  East Asia        Japan   Fukouka        NaT  33.580412  130.396361     NaN\n",
      "1  East Asia        Japan   Fukouka 1970-03-31  33.580412  130.396361     NaN\n",
      "2  East Asia       Taiwan   Unknown 1971-11-20  23.583333  119.583330     NaN\n",
      "3  East Asia        Japan  Yokosuka 1973-04-25  35.281341  139.672200     NaN\n",
      "4  East Asia        Japan     Tokyo 1973-08-29  35.689125  139.747742     NaN\n",
      "5  East Asia        Japan     Itami 1974-07-15  34.784306  135.400947     NaN\n",
      "6  East Asia  South Korea     Seoul 1974-08-15  37.566535  126.977969     NaN\n",
      "7  East Asia        Japan     Tokyo 1974-11-14  35.689125  139.747742     NaN\n",
      "8  East Asia        Japan   Fukouka 1974-11-22  33.580412  130.396361     NaN\n",
      "9  East Asia        Japan     Tokyo 1975-02-28  35.689125  139.747742     NaN\n"
     ]
    }
   ],
   "source": [
    "# Count how many columns are in the csv dataset after the columns to keep has been applied\n",
    "col_count = len(df.columns)\n",
    "print(\"Total number of columns:\", col_count)\n",
    "\n",
    "# Show the first 10 rows to check the columns have been removed from the data frame\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "092db18f-6271-4a90-83a2-e012242257fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 8 rows with missing coordinates.\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing latitude or longitude - as points need to be plotted\n",
    "#if 'latitude' in df and 'longitude' in df:\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['latitude', 'longitude'])\n",
    "after = len(df)\n",
    "print(f\"Removed {before - after} rows with missing coordinates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa1070a1-2f25-4c2e-83a8-c62044b33ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of rows: 839\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows to show how many incidents there are\n",
    "print(\"\\nTotal number of rows:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4e85c30-7874-473f-9dad-746b059e52e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase text for all the columns to make handelling text easier if needed\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "        df[col] = df[col].str.lower().str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5521595-e789-4607-9e05-1ff954e83d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of rows: 839\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of rows do see how many duplicate records there were\n",
    "print(\"\\nTotal number of rows:\", len(df))\n",
    "\n",
    "# drop duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea22e2f-70ab-4f90-93ce-a5949ccbc685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  region_txt country_txt      city       date   latitude   longitude summary\n",
      "0  east asia       japan   fukouka        NaT  33.580412  130.396361     NaN\n",
      "1  east asia       japan   fukouka 1970-03-31  33.580412  130.396361     NaN\n",
      "2  east asia      taiwan   unknown 1971-11-20  23.583333  119.583330     NaN\n",
      "3  east asia       japan  yokosuka 1973-04-25  35.281341  139.672200     NaN\n",
      "4  east asia       japan     tokyo 1973-08-29  35.689125  139.747742     NaN\n"
     ]
    }
   ],
   "source": [
    "# Show a few rows to check if text is lowercased\n",
    "print(df.head(5))  # Shows first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "152a396a-0b27-4da6-9d2b-91ee5174971d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPSG:4326\n"
     ]
    }
   ],
   "source": [
    "# Check CRS to ensure that the correct projection is being used in the mapping and set to\n",
    "print(gdf.crs)\n",
    "\n",
    "# If not EPSG:4326, reproject it:\n",
    "if gdf.crs != 'EPSG:4326':\n",
    "    gdf = gdf.to_crs(epsg=4326)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c43f84d2-3388-4653-b9fd-d4ecb181b9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open file to get the shapefile of the Hong Kong area - shapefile made by martin humphrey as part of the assigment\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "file_path = filedialog.askopenfilename(\n",
    "    title=\"select your shapefile\",\n",
    "    filetypes=[(\"Shapefiles\", \"*.shp\")]\n",
    ")\n",
    "\n",
    "# load the selected shapefile of the countries. This uses geopandas, and encodes into latin1, could use UTF8\n",
    "gdf = gpd.read_file(file_path, encoding='latin1')\n",
    "\n",
    "# use the existing df_attacks (assumed already loaded and cleaned)\n",
    "# create a folium map and center it based on the shapefile\n",
    "bounds = gdf.total_bounds  # [minx, miny, maxx, maxy]\n",
    "center_lat = (bounds[1] + bounds[3]) / 2\n",
    "center_lon = (bounds[0] + bounds[2]) / 2\n",
    "\n",
    "# make the map using folium\n",
    "m = folium.Map(location=[center_lat, center_lon], zoom_start=3)\n",
    "\n",
    "# add shapefile features\n",
    "for idx, row in gdf.iterrows():\n",
    "    geom = row['geometry']\n",
    "    \n",
    "    if isinstance(geom, Point):\n",
    "        folium.CircleMarker(\n",
    "            location=[geom.y, geom.x],\n",
    "            radius=3,\n",
    "            color='blue',\n",
    "            fill=True,\n",
    "            fill_opacity=0.6,\n",
    "            popup=f\"point - {row.get('city', 'unknown')}\"\n",
    "        ).add_to(m)\n",
    "        \n",
    "    elif isinstance(geom, Polygon):\n",
    "        folium.Polygon(\n",
    "            locations=[(coord[1], coord[0]) for coord in geom.exterior.coords],\n",
    "            color='green',\n",
    "            fill=True,\n",
    "            fill_opacity=0.4,\n",
    "            popup=\"polygon area\"\n",
    "        ).add_to(m)\n",
    "        \n",
    "    elif isinstance(geom, MultiPolygon):\n",
    "        for poly in geom:\n",
    "            folium.Polygon(\n",
    "                locations=[(coord[1], coord[0]) for coord in poly.exterior.coords],\n",
    "                color='purple',\n",
    "                fill=True,\n",
    "                fill_opacity=0.4,\n",
    "                popup=\"multipolygon area\"\n",
    "            ).add_to(m)\n",
    "\n",
    "# add attack locations on the same map\n",
    "for idx, row in df.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row['latitude'], row['longitude']],\n",
    "        radius=1,\n",
    "        color='yellow',\n",
    "        fill=True,\n",
    "        fill_opacity=0.7,\n",
    "        popup=folium.Popup(\n",
    "            f\"city: {row.get('city', 'unknown')}<br>\"\n",
    "            f\"country: {row.get('country', 'unknown')}<br>\"\n",
    "            f\"date: {row.get('date', 'unknown')}\",\n",
    "            max_width=300\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "# save the combined map\n",
    "output_map = \"generated_map.html\"\n",
    "m.save(output_map)\n",
    "\n",
    "# open the html map automatically\n",
    "webbrowser.open(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03d27546-8da0-443d-8a37-6501a4469544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable          Type             Data/Info\n",
      "--------------------------------------------\n",
      "LineString        type             <class 'shapely.geometry.linestring.LineString'>\n",
      "MultiPolygon      type             <class 'shapely.geometry.<...>ltipolygon.MultiPolygon'>\n",
      "NamespaceMagics   MetaHasTraits    <class 'IPython.core.magi<...>mespace.NamespaceMagics'>\n",
      "Point             type             <class 'shapely.geometry.point.Point'>\n",
      "Polygon           type             <class 'shapely.geometry.polygon.Polygon'>\n",
      "after             int              839\n",
      "before            int              847\n",
      "bounds            ndarray          4: 4 elems, type `float64`, 32 bytes\n",
      "center_lat        float64          21.80419344175928\n",
      "center_lon        float64          110.50732488194376\n",
      "col               str              summary\n",
      "col_count         int              7\n",
      "columns_to_keep   list             n=7\n",
      "df                DataFrame            region_txt country_tx<...>n\\n[694 rows x 7 columns]\n",
      "earthaccess       module           <module 'earthaccess' fro<...>arthaccess\\\\__init__.py'>\n",
      "file              str              C:/Users/marti/Downloads/east asia terrorism.csv\n",
      "file_path         str              C:/Users/marti/Downloads/<...>t_converted_converted.shp\n",
      "filedialog        module           <module 'tkinter.filedial<...>\\tkinter\\\\filedialog.py'>\n",
      "folium            module           <module 'folium' from 'C:<...>es\\\\folium\\\\__init__.py'>\n",
      "gdf               GeoDataFrame         id         Name  desc<...>7195 0, 114.34411 22...  \n",
      "geom              Polygon          POLYGON Z ((114.244345438<...>6807 22.5719524150113 0))\n",
      "get_ipython       function         <function get_ipython at 0x000001DFD2DBB4C0>\n",
      "gpd               module           <module 'geopandas' from <...>\\geopandas\\\\__init__.py'>\n",
      "idx               int              846\n",
      "json              module           <module 'json' from 'C:\\\\<...>\\Lib\\\\json\\\\__init__.py'>\n",
      "m                 Map              <folium.folium.Map object at 0x000001DFDC536490>\n",
      "mapping           function         <function mapping at 0x000001DFD77911C0>\n",
      "orient            function         <function orient at 0x000001DFD77900E0>\n",
      "os                module           <module 'os' from 'C:\\\\Us<...>ial_cluster\\\\Lib\\\\os.py'>\n",
      "output_map        str              generated_map.html\n",
      "pd                module           <module 'pandas' from 'C:<...>es\\\\pandas\\\\__init__.py'>\n",
      "plt               module           <module 'matplotlib.pyplo<...>\\\\matplotlib\\\\pyplot.py'>\n",
      "rasterio          module           <module 'rasterio' from '<...>\\\\rasterio\\\\__init__.py'>\n",
      "root              Tk               .\n",
      "row               Series           region_txt               <...>nName: 846, dtype: object\n",
      "row_count         int              847\n",
      "shapely           module           <module 'shapely' from 'C<...>s\\\\shapely\\\\__init__.py'>\n",
      "show              function         <function show at 0x000001DFDC04E3E0>\n",
      "sys               module           <module 'sys' (built-in)>\n",
      "tk                module           <module 'tkinter' from 'C<...>b\\\\tkinter\\\\__init__.py'>\n",
      "webbrowser        module           <module 'webbrowser' from<...>ter\\\\Lib\\\\webbrowser.py'>\n"
     ]
    }
   ],
   "source": [
    "#overview of all the variables within the memory\n",
    "%whos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1e3d5d4-039b-4d2a-a9bd-f459dc0463d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open file to get the shapefile of the countries\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "file_path = filedialog.askopenfilename(\n",
    "    title=\"select your shapefile\",\n",
    "    filetypes=[(\"Shapefiles\", \"*.shp\")]\n",
    ")\n",
    "\n",
    "if file_path:\n",
    "    # read the shapefile into a GeoDataFrame\n",
    "    countries = gpd.read_file(file_path).to_crs(epsg=4326)\n",
    "\n",
    "    # get the union of all geometries\n",
    "    outline = countries.geometry.union_all()\n",
    "\n",
    "    # wrap it in a GeoDataFrame so Jupyter will plot it\n",
    "    outline_gdf = gpd.GeoDataFrame(geometry=[outline], crs=\"EPSG:4326\")\n",
    "\n",
    "    # display the outline\n",
    "    outline_gdf.explore()  # if you want interactive (needs folium)\n",
    "    # or:\n",
    "    # outline_gdf.plot()    # for static plot\n",
    "else:\n",
    "    print(\"No file selected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d186c10-b2ce-4d55-8724-5476f7544119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74.1572055810097, -9.51660313206516, 146.8574441828778, 53.12499001558372)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline.bounds # get the min x, min y, max x, max y values of the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6276ae0a-d079-45ae-b633-bca85f29cc01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"58.709153506566096 -17.69940340658555 91.53860954847754 90.51952110357408\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,55.12071429040299)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"1.8307721909695507\" opacity=\"0.6\" d=\"M 146.8574441828778,43.88707991065738 L 122.07079115012486,-14.309084534419716 L 62.09947237873193,11.233634379745455 L 86.8861254114849,69.4297988248227 L 146.8574441828778,43.88707991065738 z\" /></g></svg>"
      ],
      "text/plain": [
       "<POLYGON ((146.857 43.887, 122.071 -14.309, 62.099 11.234, 86.886 69.43, 146...>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gets the minimum rotated rectangle that covers the outline. Shapely geometry property that calculates the smallest rectangle (by area) \n",
    "# that can fully contain the polygon, but unlike a bounding box, it’s allowed to rotate.\n",
    "search_area = outline.minimum_rotated_rectangle\n",
    "\n",
    "search_area # in a jupyter notebook, this displays the polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e16e0dab-e5ee-4a8c-9774-fc8ec6e0a387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"58.709153506566096 -17.69940340658555 91.53860954847754 90.51952110357408\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,55.12071429040299)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"1.8307721909695507\" opacity=\"0.6\" d=\"M 146.8574441828778,43.88707991065738 L 86.8861254114849,69.4297988248227 L 62.09947237873193,11.233634379745455 L 122.07079115012486,-14.309084534419716 L 146.8574441828778,43.88707991065738 z\" /></g></svg>"
      ],
      "text/plain": [
       "<POLYGON ((146.857 43.887, 86.886 69.43, 62.099 11.234, 122.071 -14.309, 146...>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_area = shapely.geometry.polygon.orient(search_area, sign=1) # a sign of 1 means oriented counter-clockwise this is important\n",
    "\n",
    "search_area # this doesn't actually change the geometry, just the order of the vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e97c935-fea3-42e4-bf1c-22cc6f714c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<earthaccess.auth.Auth at 0x1dfdbb452b0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#login to get the earthaccess data\n",
    "earthaccess.login(strategy='.netrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a818d906-52cc-4222-a774-0bc9db668564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 634 dataset(s) related to elevation.\n",
      "→ GLAS/ICESat L1B Global Elevation Data (HDF5) V034\n",
      "→ GLAS/ICESat L1B Global Elevation Data (HDF5) V034\n",
      "→ ASTER Global Digital Elevation Model V003\n",
      "→ GEDI L2A Elevation and Height Metrics Data Global Footprint Level V002\n",
      "→ CryoSat-2 Level-1B Waveforms, Sea Ice Elevation, and Surface Roughness V001\n",
      "→ ASTER Global Digital Elevation Model NetCDF V003\n",
      "→ CryoSat-2 Level-4 Sea Ice Elevation, Freeboard, and Thickness V001\n",
      "→ ASTER Digital Elevation Model V003\n",
      "→ ASTER Orthorectified Digital Elevation Model (DEM) V003\n",
      "→ IceBridge ATM L2 Icessn Elevation, Slope, and Roughness V002\n"
     ]
    }
   ],
   "source": [
    "#seach for the dataset that you want that cover the area \n",
    "datasets = earthaccess.search_datasets(\n",
    "    keyword='elevation', # search for datasets that match the keyword 'elevation'\n",
    "    polygon=search_area.exterior.coords # search for datasets that intersect Hong Kong and Macau\n",
    ")\n",
    "\n",
    "if datasets:\n",
    "    print(f\"Found {len(datasets)} dataset(s) related to elevation.\")\n",
    "    for ds in datasets[:10]:  # Show first 10 dataset names\n",
    "        print(\"→\", ds[\"umm\"][\"EntryTitle\"])\n",
    "else:\n",
    "    print(\"No datasets found in the selected area.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ac945d6-b2d4-4559-95fb-6809ee265873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Entry Title:\n",
      "ASTER Global Digital Elevation Model V003\n",
      "\n",
      "Short Name:\n",
      "ASTGTM\n",
      "\n",
      "Abstract:\n",
      "The ASTER Global Digital Elevation Model (GDEM) Version 3 (ASTGTM) provides a global digital elevation model (DEM) of land areas on Earth at a spatial resolution of 1 arc second (approximately 30 meter horizontal posting at the equator).\n",
      "\n",
      "The development of the ASTER GDEM data products is a collaborative effort between National Aeronautics and Space Administration (NASA) and Japan’s Ministry of Economy, Trade, and Industry (METI). The ASTER GDEM data products are created by the Sensor Information Laboratory Corporation (SILC) in Tokyo. \n",
      "\n",
      "The ASTER GDEM Version 3 data product was created from the automated processing of the entire ASTER Level 1A (https://doi.org/10.5067/ASTER/AST_L1A.003) archive of scenes acquired between March 1, 2000, and November 30, 2013. Stereo correlation was used to produce over one million individual scene based ASTER DEMs, to which cloud masking was applied. All cloud screened DEMs and non-cloud screened DEMs were stacked. Residual bad values and outliers were removed. In areas with limited data stacking, several existing reference DEMs were used to supplement ASTER data to correct for residual anomalies. Selected data were averaged to create final pixel values before partitioning the data into 1 degree latitude by 1 degree longitude tiles with a one pixel overlap. To correct elevation values of water body surfaces, the ASTER Global Water Bodies Database (ASTWBD) (https://doi.org/10.5067/ASTER/ASTWBD.001) Version 1 data product was also generated. \n",
      "\n",
      "The geographic coverage of the ASTER GDEM extends from 83° North to 83° South. Each tile is distributed in GeoTIFF format and projected on the 1984 World Geodetic System (WGS84)/1996 Earth Gravitational Model (EGM96) geoid. Each of the 22,912 tiles in the collection contain at least 0.01% land area. \n",
      "\n",
      "Provided in the ASTER GDEM product are layers for DEM and number of scenes (NUM). The NUM layer indicates the number of scenes that were processed for each pixel and the source of the data.\n",
      "\n",
      "While the ASTER GDEM Version 3 data products offer substantial improvements over Version 2, users are advised that the products still may contain anomalies and artifacts that will reduce its usability for certain applications. \n",
      "\n",
      "Improvements/Changes from Previous Versions \n",
      "• Expansion of acquisition coverage to increase the amount of cloud-free input scenes from about 1.5 million in Version 2 to about 1.88 million scenes in Version 3.\n",
      "• Separation of rivers from lakes in the water body processing. \n",
      "• Minimum water body detection size decreased from 1 km2 to 0.2 km2. \n"
     ]
    }
   ],
   "source": [
    "dataset = datasets[2] # get the first 2 results\n",
    "#Get key metadata fields from the dataset\n",
    "entry_title = dataset.get_umm('EntryTitle')         # Full name of the dataset\n",
    "short_name = dataset.get_umm('ShortName')           #  dataset code\n",
    "abstract = dataset.get_umm('Abstract')              # A description of what the dataset is about\n",
    "\n",
    "# Print them out\n",
    "print(\"Dataset Entry Title:\")\n",
    "print(entry_title)\n",
    "print(\"\\nShort Name:\")\n",
    "print(short_name)\n",
    "print(\"\\nAbstract:\")\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc1da736-e22a-4ddb-922c-9d532ee92d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name=\"ASTGTM\",  # search for ASTER GDEM v3 granules\n",
    "    polygon=search_area.exterior.coords,  # search for images that intersect our search_area\n",
    "    count=10  # only show the first 10 results\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0710f4a4-05f3-4a44-bc9e-551f3650f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results) # show the length of the results list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c5f26c-03a7-4fe2-8993-ddf37a93a517",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    short_name='ASTGTM', # search for ASTER GDEM v3 granules\n",
    "    polygon=search_area.exterior.coords, # search for images that intersect our search_area\n",
    "    count=10 # only show the first 10 results\n",
    ")\n",
    "\n",
    "# Print a summary for each granule\n",
    "for i, granule in enumerate(results, start=1):\n",
    "    granule_ur = granule.get_umm('GranuleUR')  # unique granule identifier\n",
    "    temporal = granule.get_umm('TemporalExtent') or {}\n",
    "    start_time = temporal.get('BeginningDateTime', 'Unknown')\n",
    "    end_time = temporal.get('EndingDateTime', 'Unknown')\n",
    "    links = granule.data_links()\n",
    "    download_url = links[0] if links else 'No download URL available'\n",
    "\n",
    "    print(f\"\\n🔹 Granule {i}\")\n",
    "    print(f\"Granule ID: {granule_ur}\")\n",
    "    print(f\"Time Range: {start_time} to {end_time}\")\n",
    "    print(f\"Download URL: {download_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71195c0-56b1-49ef-8504-bc530682ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthaccess.login(strategy='.netrc')\n",
    "granule = next(iter(results)) # get the \"first\" item from the list\n",
    "granule # show the first item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a93b90-c725-46d2-be10-33efbe4dacd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#earthaccess.login(strategy='.netrc')\n",
    "downloaded_files = earthaccess.download(results, 'ASTGTM') # download each of the granules to the aster_gdem directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4820b3c5-b6bf-40ab-bd61-915cd1b8cbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total files downloaded: {len(downloaded_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e229b575-b1be-4523-ae10-0bde101a6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the files \n",
    "downloaded_files = earthaccess.download(results, \"./ASTGTM\")\n",
    "\n",
    "# print the list of file paths\n",
    "for file in downloaded_files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad52ada-75d3-4a05-ab04-59528aa86df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#below this add the code for loading and exploiting the imagery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949995ce-9708-4a68-bc52-6ecbf2ac1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by hiding the default Tkinter window \n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "# Open a file dialog to load the GEOTif file\n",
    "file_path = filedialog.askopenfilename(\n",
    "    title=\"Select a GeoTIFF file\",\n",
    "    filetypes=[(\"TIFF files\", \"*.tif *.tiff\")]\n",
    ")\n",
    "\n",
    "# If you dont pick a file, stop here\n",
    "if not file_path:\n",
    "    print(\"No file selected.\")\n",
    "else:\n",
    "    try:\n",
    "        # Open the .tif file using rasterio (reads geospatial raster data)\n",
    "        with rasterio.open(file_path) as dataset:\n",
    "            print(f\"{dataset.name} opened in {dataset.mode} mode\")\n",
    "            \n",
    "            # Check how many bands are in the image\n",
    "            print(f\"Image has {dataset.count} band(s)\")\n",
    "            \n",
    "            # Get the image size in pixels to understand the image\n",
    "            print(f\"Image size (width, height): {dataset.width} x {dataset.height}\")\n",
    "            \n",
    "            # Check what kind of data the first band uses (e.g., uint8, float32)\n",
    "            print(f\"Band 1 data type is {dataset.dtypes[0]}\")\n",
    "            \n",
    "            # Print coordinate system (tells us how the image lines up with real-world locations)\n",
    "            print(f\"CRS: {dataset.crs}\")\n",
    "            \n",
    "            # Print the bounding box of the image in map coordinates\n",
    "            print(f\"Bounds: {dataset.bounds}\")\n",
    "\n",
    "            # Load the first band and plot it as a grayscale image\n",
    "            band1 = dataset.read(1)\n",
    "\n",
    "            plt.imshow(band1, cmap='gray')\n",
    "            plt.title(\"Band 1 - Grayscale View\")\n",
    "            plt.colorbar(label='Pixel Values')\n",
    "            plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch any errors (e.g., if the file isn’t a proper raster)\n",
    "        print(f\"Error opening file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273df31-99c6-49cc-b89d-746a1f2d5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acknowledgement of AI and Source Material Use\n",
    "#\n",
    "# This code was developed as part of coursework for the University of Ulster.\n",
    "# It draws upon methods and concepts introduced during lectures and practical sessions.\n",
    "#\n",
    "# Prior to this module, I had no prior experience with coding.\n",
    "# I used AI assistance (specifically OpenAI’s ChatGPT) to help understand,\n",
    "# structure, and refine parts of the code and explanations.\n",
    "#\n",
    "# While I do not claim full authorship of all content, I have engaged critically with the outputs,\n",
    "# made appropriate modifications, and ensured I understand the code and techniques used.\n",
    "#\n",
    "# The AI was used as a learning aid to support my progress and enhance my understanding of geospatial coding,\n",
    "# allowing me to achieve more than would otherwise have been possible.\n",
    "# This use is consistent with the spirit of academic integrity and the policies of the University of Ulster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f917d4d-280b-46cb-ad09-0af9c1caf606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spatial_cluster)",
   "language": "python",
   "name": "spatial_cluster"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
